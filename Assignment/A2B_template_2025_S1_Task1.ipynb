{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Producing the data\n",
    "In this task, we will implement one Apache Kafka producer to simulate real-time data streaming. Spark is not allowed in this part since it’s simulating a streaming data source.\n",
    "\n",
    "1) Your program should send browsing behaviour data batch-by-batch to the Kafka stream. One batch consists of a 200 rows from the browsing behaviour dataset. The CSV shouldn’t be loaded to memory at once to conserve memory (i.e. Read row as needed).\n",
    "2) For each row, add a timestamp column named ‘ts’ in Unix timestamp format. This is the event time when you send the streaming data, which should be in int format. The ‘ts’ should be the same for the data sent in one batch.\n",
    "3) Read the transaction data rows that fall between the start and end event_time of the browsing behaviour data recorded in task 1.1, and create a batch. Similarly, as in task 1.2, add the column named ‘ts’ for each row.\n",
    "4) At an interval of 1 second, send the two data batches consecutively (browsing behaviour data & transaction data) from 1.1 and 1.3 to respective Kafka topics with an appropriate name.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
